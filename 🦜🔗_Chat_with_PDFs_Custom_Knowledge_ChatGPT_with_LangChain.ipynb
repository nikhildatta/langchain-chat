{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nikhildatta/langchain-chat/blob/main/%F0%9F%A6%9C%F0%9F%94%97_Chat_with_PDFs_Custom_Knowledge_ChatGPT_with_LangChain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Custom Knowledge ChatGPT with LangChain - Chat with PDFs**\n",
        "\n",
        "**By Liam Ottley:**  [YouTube](https://youtube.com/@LiamOttley)\n",
        "\n",
        "0.   Installs, Imports and API Keys\n",
        "1.   Loading PDFs and chunking with LangChain\n",
        "2.   Embedding text and storing embeddings\n",
        "3.   Creating retrieval function\n",
        "4.   Creating chatbot with chat memory"
      ],
      "metadata": {
        "id": "_x1GI7Fo8Y7x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Installs, Imports and API Keys"
      ],
      "metadata": {
        "id": "Q24Y-g6h-Bg0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RUN THIS CELL FIRST!\n",
        "!pip install -q langchain==0.0.150 pypdf pandas matplotlib tiktoken textract transformers openai faiss-cpu tensorflow_text==2.8.2"
      ],
      "metadata": {
        "id": "gk2J2sYYjTkM",
        "outputId": "dc57844f-1984-455d-ea4e-5e8d20e3a2d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m498.1/498.1 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.3/462.3 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-datasets 4.9.2 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\n",
            "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import GPT2TokenizerFast\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
        "# from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.embeddings import TensorflowHubEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.document_transformers import EmbeddingsRedundantFilter\n",
        "from langchain.retrievers.document_compressors import DocumentCompressorPipeline, EmbeddingsFilter\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "path = \"/content/drive/MyDrive/Colab Notebooks/\""
      ],
      "metadata": {
        "id": "l-uszlwN641q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ec6e5f9-55f7-4967-f118-0c9250aece23"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
        "url = path + 'universal-sentence-encoder/' # 'https://tfhub.dev/google/universal-sentence-encoder/4'"
      ],
      "metadata": {
        "id": "E2Buv5Y0uFr8"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Loading PDFs and chunking with LangChain"
      ],
      "metadata": {
        "id": "RLULMPXa-Hu8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split by chunk\n",
        "\n",
        "# Step 1: Convert PDF to text\n",
        "import textract\n",
        "doc = textract.process(path + \"/attention_is_all_you_need.pdf\")\n",
        "\n",
        "# Step 2: Save to .txt and reopen (helps prevent issues)\n",
        "with open('attention_is_all_you_need.txt', 'w') as f:\n",
        "    f.write(doc.decode('utf-8'))\n",
        "\n",
        "with open('attention_is_all_you_need.txt', 'r') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# Step 3: Create function to count tokens\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
        "\n",
        "def count_tokens(text: str) -> int:\n",
        "    return len(tokenizer.encode(text))\n",
        "\n",
        "# Step 4: Split text into chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    # Set a really small chunk size, just to show.\n",
        "    chunk_size = 300,\n",
        "    chunk_overlap  = 20,\n",
        "    length_function = count_tokens,\n",
        "    separators = \". \"\n",
        ")\n",
        "\n",
        "chunks = text_splitter.create_documents([text])"
      ],
      "metadata": {
        "id": "iADY2CXNlNq9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Embed text and store embeddings"
      ],
      "metadata": {
        "id": "_IlznUDK-i2m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get embedding model\n",
        "embeddings = TensorflowHubEmbeddings(model_url=url)\n",
        "\n",
        "# Create vector database\n",
        "db = FAISS.from_documents(chunks, embeddings)"
      ],
      "metadata": {
        "id": "92ObhTAKnZzQ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Setup retrieval function"
      ],
      "metadata": {
        "id": "2LPwdGDP-nPO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check similarity search is working\n",
        "query = \"Who created transformers?\"\n",
        "docs = db.similarity_search(query, similarity_threshold=0.76)\n",
        "docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWP92zGg5Nb_",
        "outputId": "7a7c8fee-b776-4625-8d3b-02e2a56c1b81"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [14, 15] and [8].\\n\\n3 Model Architecture\\n\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 29].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[9], consuming the previously generated symbols as additional input when generating the next.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n\\n3.1 Encoder and Decoder Stacks\\n\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The ﬁrst is a multi-head self-attention mechanism, and the second is a simple, position-\\n\\n2\\n\\n\\x0cFigure 1: The Transformer - model architecture', metadata={}),\n",
              " Document(page_content='.., headh)W O\\n\\nwhere headi = Attention(QW Q\\n\\ni , KW K\\ni\\n\\n, V W V\\n\\ni )\\n\\nWhere the projections are parameter matrices W Q\\nand W O ∈ Rhdv×dmodel.\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n\\ni ∈ Rdmodel×dk, W K\\n\\ni ∈ Rdmodel×dk, W V\\n\\ni ∈ Rdmodel×dv\\n\\n3.2.3 Applications of Attention in our Model\\n\\nThe Transformer uses multi-head attention in three different ways:\\n\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[31, 2, 8].\\n• The encoder contains self-attention layers', metadata={}),\n",
              " Document(page_content='Ashish, with Illia, designed and implemented the ﬁrst Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefﬁcient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\n\\n\\x0cRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples', metadata={}),\n",
              " Document(page_content='For translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\n\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\n\\n9\\n\\n\\x0cReferences\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\n\\narXiv:1607.06450, 2016.\\n\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio', metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Create chatbot with chat memory"
      ],
      "metadata": {
        "id": "U_nH1qoL-w--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create conversation chain that uses our vectordb as retriver, this also allows for chat history management\n",
        "relevant_retriever = db.as_retriever()\n",
        "splitter = CharacterTextSplitter(chunk_size=250, chunk_overlap=0, separator=\". \")\n",
        "redundant_filter = EmbeddingsRedundantFilter(embeddings=embeddings)\n",
        "relevant_filter = EmbeddingsFilter(embeddings=embeddings, similarity_threshold=0.76)\n",
        "pipeline_compressor = DocumentCompressorPipeline(\n",
        "    transformers=[ splitter, redundant_filter, relevant_filter]\n",
        ")\n",
        "qa = ConversationalRetrievalChain.from_llm(ChatOpenAI(temperature=0.2), relevant_retriever)"
      ],
      "metadata": {
        "id": "evF7_Dyhtcaf"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history = []\n",
        "\n",
        "print(\"Welcome to the Transformers chatbot! Type 'exit' to stop.\")\n",
        "query = input(\"User:\\t\\t\")\n",
        "\n",
        "while query.lower() != \"exit\":\n",
        "\n",
        "    result = qa({\"question\": query, \"chat_history\": chat_history})\n",
        "    chat_history.append((query, result['answer']))\n",
        "\n",
        "    print('Chatbot:\\t{answer}\\n'.format(answer = result[\"answer\"]))\n",
        "    query = input(\"User:\\t\\t\")\n",
        "\n",
        "print(\"Thank you for using the Attention Transformer chatbot!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pHw5siewPNt",
        "outputId": "ea4118df-611e-4ee7-c79d-fef8c4fa15c5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to the Transformers chatbot! Type 'exit' to stop.\n",
            "User:\t\twho created transformers?\n",
            "Chatbot:\tThe Transformer model was designed and implemented by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin.\n",
            "\n",
            "User:\t\texplain transformers in 100 words\n",
            "Chatbot:\tTransformers are a type of neural network architecture used for sequence transduction tasks, such as machine translation. They consist of an encoder and a decoder, each composed of multiple identical layers. The key component of transformers is self-attention, which allows the model to focus on different parts of the input sequence when generating the output. This attention mechanism enables transformers to capture long-range dependencies and improve performance compared to traditional models. Transformers have achieved state-of-the-art results in various natural language processing tasks and are known for their parallelizability and faster training speed.\n",
            "\n",
            "User:\t\texplain transformers in 100 words using only relevant details.\n",
            "Chatbot:\tThe key components of transformers are multi-head attention and position-wise feed-forward networks. \n",
            "\n",
            "Multi-head attention allows the model to focus on different parts of the input sequence simultaneously. It consists of multiple attention heads, each performing its own attention calculation. This allows the model to capture different types of information and dependencies in the input sequence.\n",
            "\n",
            "Position-wise feed-forward networks are fully connected layers applied to each position separately and identically. They help the model to capture complex non-linear relationships between different positions in the sequence.\n",
            "\n",
            "These components improve performance compared to traditional models in several ways:\n",
            "\n",
            "1. Parallelization: The use of multi-head attention allows for parallel computation, as each attention head can be computed independently. This speeds up training and inference, making the model more efficient.\n",
            "\n",
            "2. Long-range dependencies: Transformers can capture long-range dependencies in the input sequence more effectively compared to traditional models. This is because each position in the sequence can attend to all other positions, allowing for better information flow.\n",
            "\n",
            "3. Scalability: Transformers scale well to larger datasets and longer sequences. Traditional models, such as recurrent neural networks, struggle with long sequences due to the sequential nature of their computations. Transformers, on the other hand, can process sequences in parallel, making them more suitable for tasks with long input sequences.\n",
            "\n",
            "4. Interpretability: The attention mechanism in transformers provides interpretability, as it allows us to understand which parts of the input sequence the model is attending to. This can be useful for understanding the model's decision-making process and debugging.\n",
            "\n",
            "Overall, the key components of transformers enable better modeling of dependencies, parallelization, scalability, and interpretability, leading to improved performance compared to traditional models.\n",
            "\n",
            "User:\t\texplain transformers in 100 words using only relevant details.\n",
            "Chatbot:\tThe key components of transformers are:\n",
            "\n",
            "1. Self-Attention Mechanism: This mechanism allows each position in the input sequence to attend to all other positions, capturing dependencies between different words. It replaces the sequential nature of traditional models, such as recurrent neural networks (RNNs), with parallel processing.\n",
            "\n",
            "2. Multi-Head Attention: Transformers use multiple parallel attention layers, or heads, to capture different types of information and learn different representations. Each head attends to different parts of the input sequence, allowing the model to capture different types of relationships and dependencies.\n",
            "\n",
            "3. Positional Encoding: Since transformers do not have any inherent notion of word order, positional encoding is used to provide information about the position of each word in the input sequence. This allows the model to understand the sequential order of the words.\n",
            "\n",
            "4. Feed-Forward Neural Networks: Transformers use feed-forward neural networks to process the information from the self-attention mechanism and produce the final output. These networks consist of multiple layers of linear transformations and non-linear activation functions.\n",
            "\n",
            "Compared to traditional models, transformers have several advantages that contribute to their improved performance:\n",
            "\n",
            "1. Capturing Long-Term Dependencies: The self-attention mechanism allows transformers to capture long-term dependencies between words in the input sequence, which is challenging for traditional models like RNNs. This leads to better understanding of context and improved performance in tasks like machine translation.\n",
            "\n",
            "2. Parallel Processing: Transformers process the entire input sequence in parallel, rather than sequentially like RNNs. This makes them more efficient and allows for faster training and inference.\n",
            "\n",
            "3. Scalability: Transformers can handle input sequences of variable length without the need for padding or truncation. This makes them more flexible and suitable for tasks with long input sequences, such as document classification or language modeling.\n",
            "\n",
            "4. Interpretability: The attention mechanism in transformers provides interpretability, as it allows us to understand which parts of the input sequence are important for making predictions. This can be useful for tasks where interpretability is important, such as natural language understanding or question answering.\n",
            "\n",
            "Overall, the combination of self-attention, multi-head attention, and feed-forward neural networks in transformers allows them to capture complex relationships in the input sequence and achieve state-of-the-art performance in various natural language processing tasks.\n",
            "\n",
            "User:\t\tcan you arrange the names in alphabetical order?\n",
            "Chatbot:\tThe creators of the Transformer model are:\n",
            "\n",
            "- Aidan N. Gomez\n",
            "- Ashish Vaswani\n",
            "- Illia Polosukhin\n",
            "- Jakob Uszkoreit\n",
            "- Llion Jones\n",
            "- Łukasz Kaiser\n",
            "- Niki Parmar\n",
            "- Noam Shazeer\n",
            "\n",
            "User:\t\twhat is attention mechanism?\n",
            "Chatbot:\tThe attention mechanism in transformers is a key component that allows the model to focus on different parts of the input sequence when generating the output. It is used to compute a weighted sum of the values at different positions in the input sequence, based on the similarity between the query and the keys. This allows the model to give more importance to relevant parts of the input sequence and ignore irrelevant parts.\n",
            "\n",
            "In the transformer model, attention is used in two main ways:\n",
            "1. Self-attention: This is also known as intra-attention. It allows the model to relate different positions within the same sequence. By attending to different positions, the model can compute a representation of the sequence that captures dependencies between different parts.\n",
            "2. Encoder-decoder attention: This allows the decoder to attend over the encoder's output sequence. The queries come from the previous decoder layer, and the keys and values come from the encoder's output. This allows the decoder to focus on different parts of the input sequence when generating the output.\n",
            "\n",
            "The attention mechanism in transformers is based on a multi-head approach, where multiple attention heads are used to capture different types of dependencies. Each attention head computes a weighted sum of the values based on the similarity between the query and the keys. The outputs of the attention heads are then concatenated and linearly transformed to produce the final output.\n",
            "\n",
            "Overall, the attention mechanism in transformers plays a crucial role in capturing dependencies between different parts of the input sequence and generating accurate representations for the output.\n",
            "\n",
            "User:\t\texit\n",
            "Thank you for using the Attention Transformer chatbot!\n"
          ]
        }
      ]
    }
  ]
}